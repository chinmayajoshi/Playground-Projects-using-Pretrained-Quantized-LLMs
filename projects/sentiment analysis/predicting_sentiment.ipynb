{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27481, 10)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('data/train.csv', encoding='unicode_escape')\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing Tweet Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_for_llm(tweet_arr):\n",
    "    tweet_txt = tweet_arr[0][1]\n",
    "\n",
    "    tweet_info = f'''Tweet Content: {tweet_txt}'''\n",
    "\n",
    "    return tweet_info\n",
    "\n",
    "def extract_sentiment_for_llm(tweet_arr):\n",
    "    '''for providing N-shot examples for the LLM in the prompt'''\n",
    "    \n",
    "    # relevant_tokens = tweet_arr[0][2]\n",
    "    sentiment = tweet_arr[0][3]\n",
    "\n",
    "    tweet_sentiment = f'''Sentiment: [{sentiment}]'''\n",
    "\n",
    "    return tweet_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['77a409775e'\n",
      "  'wooo am recovering from running race for life yest!!! i managed 36 mins 44secs, not bad for absolutley no trianing'\n",
      "  'not bad fo' 'positive' 'noon' '21-30' 'Myanmar (formerly Burma)'\n",
      "  54409800 653290.0 83]]\n",
      "\n",
      "[['8fd6a28145'\n",
      "  ' it`s tongue in cheek of some mentalities.There is another one which takes the piss of arabs, but its too long'\n",
      "  '.There is another one which takes the piss of arabs,' 'negative'\n",
      "  'morning' '46-60' 'Mali' 20250833 1220190.0 17]]\n",
      "\n",
      "[['714d7c7a34'\n",
      "  'On train with at least two gaggles of teenagers sitting & the commuters squished standing in the back...at least the teenagers let me sit'\n",
      "  'On train with at least two gaggles of teenagers sitting & the commuters squished standing in the back...at least the teenagers let me sit'\n",
      "  'neutral' 'morning' '0-20' 'Singapore' 5850342 700.0 8358]]\n"
     ]
    }
   ],
   "source": [
    "positive_sample = tweets[tweets.sentiment == \"positive\"].sample(1).values\n",
    "print(positive_sample, end='\\n\\n')\n",
    "\n",
    "negative_sample = tweets[tweets.sentiment == \"negative\"].sample(1).values\n",
    "print(negative_sample, end='\\n\\n')\n",
    "\n",
    "neutral_sample = tweets[tweets.sentiment == \"neutral\"].sample(1).values\n",
    "print(neutral_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f'''You are an expert at linguistic analysis \\\n",
    "and your area of expertise is interpreting the sentiment of \\\n",
    "texts from tweets published online. You are very intelligent and \\\n",
    "are extremely good at your job. \n",
    "\n",
    "You task will go as follows: you will be provided \\\n",
    "with some information containing the text from a tweet.\\\n",
    "You will analyze it expertly and respond with 2 pieces of information.\n",
    "\n",
    "First, the sentiment of the tweet. It could be \\\n",
    "one of three options: [positive, negative, neutral].\\\n",
    "\n",
    "Secondly, you will point out the words \\\n",
    "from the original tweet that supports that sentiment inference \\\n",
    "in your expert and accurate analysis.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-shot example from each sentiment class in the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = system_prompt + \"\\n\\n3 examples, one from each class are given below:-\\n\"\n",
    "    \n",
    "n_shot_examples = [positive_sample, negative_sample, neutral_sample]\n",
    "\n",
    "for sample in n_shot_examples:\n",
    "\n",
    "    sample_tweet_content = extract_text_for_llm(sample)\n",
    "    sample_tweet_sentiment = extract_sentiment_for_llm(sample)\n",
    "\n",
    "    one_shot_example = f'''\\n{sample_tweet_content}\\n\\n\\\n",
    "{sample_tweet_sentiment}\\n'''\n",
    "    \n",
    "    system_prompt = system_prompt + one_shot_example + \"\\n\"\n",
    "\n",
    "system_prompt = system_prompt +\\\n",
    "'''\\nNow, you are going to be given information for a single tweet.\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at linguistic analysis and your area of expertise is interpreting the sentiment of texts from tweets published online. You are very intelligent and are extremely good at your job. \n",
      "\n",
      "You task will go as follows: you will be provided with some information containing the text from a tweet.You will analyze it expertly and respond with 2 pieces of information.\n",
      "\n",
      "First, the sentiment of the tweet. It could be one of three options: [positive, negative, neutral].\n",
      "Secondly, you will point out the words from the original tweet that supports that sentiment inference in your expert and accurate analysis.\n",
      "\n",
      "3 examples, one from each class are given below:-\n",
      "\n",
      "Tweet Content: wooo am recovering from running race for life yest!!! i managed 36 mins 44secs, not bad for absolutley no trianing\n",
      "\n",
      "Sentiment: [positive]\n",
      "\n",
      "\n",
      "Tweet Content:  it`s tongue in cheek of some mentalities.There is another one which takes the piss of arabs, but its too long\n",
      "\n",
      "Sentiment: [negative]\n",
      "\n",
      "\n",
      "Tweet Content: On train with at least two gaggles of teenagers sitting & the commuters squished standing in the back...at least the teenagers let me sit\n",
      "\n",
      "Sentiment: [neutral]\n",
      "\n",
      "\n",
      "Now, you are going to be given information for a single tweet.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_prediction(prompt):\n",
    "    return prompt.split('[')[-1].split(']')[0]\n",
    "\n",
    "get_prediction(system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading LLM Model (mistral-7b-instruct-v0.1.Q5_K_M.gguf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from C:/Users/chinm/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "mpath = r'C:/Users/chinm/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf'\n",
    "\n",
    "llm = Llama(\n",
    "        model_path=mpath, # model path\n",
    "        system_prompt=system_prompt, # system prompt\n",
    "        n_ctx=2048, # model context\n",
    "        n_gpu_layers=-1 # numper of layers to offload for GPU acceleration \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ca77a6fba2',\n",
       "        'had an answered prayer which caught me by surprise. LORD, you are amazing!',\n",
       "        'amazing!', 'positive', 'night', '31-45', 'Equatorial Guinea',\n",
       "        1402985, 28050.0, 50]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = tweets.sample(1).values\n",
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Content: had an answered prayer which caught me by surprise. LORD, you are amazing!\n",
      "\n",
      "Sentiment: [positive]\n"
     ]
    }
   ],
   "source": [
    "test_tweet_content = extract_text_for_llm(test_sample)\n",
    "print(test_tweet_content, end='\\n\\n')\n",
    "\n",
    "test_tweet_sentiment = extract_sentiment_for_llm(test_sample)\n",
    "print(test_tweet_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   19228.15 ms\n",
      "llama_print_timings:      sample time =       1.86 ms /     9 runs   (    0.21 ms per token,  4843.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19227.82 ms /   363 tokens (   52.97 ms per token,    18.88 tokens per second)\n",
      "llama_print_timings:        eval time =    1598.62 ms /     8 runs   (  199.83 ms per token,     5.00 tokens per second)\n",
      "llama_print_timings:       total time =   20854.83 ms /   371 tokens\n"
     ]
    }
   ],
   "source": [
    "question_prompt = system_prompt + test_tweet_content + \\\n",
    "'''\\nPredict the sentiment class from positive/negative/neutral.\n",
    "Do not continue after making the prediction for this single tweet.\\n\n",
    "Sentiment for '''\n",
    "\n",
    "output = llm(\n",
    "    question_prompt, # Prompt\n",
    "    max_tokens= 50, # Generate up to [max_tokens] tokens, set to None to generate up to the end of the context window\n",
    "    echo=True, # Echo the prompt back in the output\n",
    "    temperature=0, # The temperature parameter (higher for more \"creative\" or \"out of distribution\" output)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-eed2aec7-479d-4709-aa4a-a7761af32831',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1715923385,\n",
       " 'model': 'C:/Users/chinm/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf',\n",
       " 'choices': [{'text': 'You are an expert at linguistic analysis and your area of expertise is interpreting the sentiment of texts from tweets published online. You are very intelligent and are extremely good at your job. \\n\\nYou task will go as follows: you will be provided with some information containing the text from a tweet.You will analyze it expertly and respond with 2 pieces of information.\\n\\nFirst, the sentiment of the tweet. It could be one of three options: [positive, negative, neutral].\\nSecondly, you will point out the words from the original tweet that supports that sentiment inference in your expert and accurate analysis.\\n\\n3 examples, one from each class are given below:-\\n\\nTweet Content: wooo am recovering from running race for life yest!!! i managed 36 mins 44secs, not bad for absolutley no trianing\\n\\nSentiment: [positive]\\n\\n\\nTweet Content:  it`s tongue in cheek of some mentalities.There is another one which takes the piss of arabs, but its too long\\n\\nSentiment: [negative]\\n\\n\\nTweet Content: On train with at least two gaggles of teenagers sitting & the commuters squished standing in the back...at least the teenagers let me sit\\n\\nSentiment: [neutral]\\n\\n\\nNow, you are going to be given information for a single tweet.\\nTweet Content: had an answered prayer which caught me by surprise. LORD, you are amazing!\\nPredict the sentiment class from positive/negative/neutral.\\nDo not continue after making the prediction for this single tweet.\\n\\nSentiment for 1 tweet: [positive]',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 363, 'completion_tokens': 8, 'total_tokens': 371}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at linguistic analysis and your area of expertise is interpreting the sentiment of texts from tweets published online. You are very intelligent and are extremely good at your job. \n",
      "\n",
      "You task will go as follows: you will be provided with some information containing the text from a tweet.You will analyze it expertly and respond with 2 pieces of information.\n",
      "\n",
      "First, the sentiment of the tweet. It could be one of three options: [positive, negative, neutral].\n",
      "Secondly, you will point out the words from the original tweet that supports that sentiment inference in your expert and accurate analysis.\n",
      "\n",
      "3 examples, one from each class are given below:-\n",
      "\n",
      "Tweet Content: wooo am recovering from running race for life yest!!! i managed 36 mins 44secs, not bad for absolutley no trianing\n",
      "\n",
      "Sentiment: [positive]\n",
      "\n",
      "\n",
      "Tweet Content:  it`s tongue in cheek of some mentalities.There is another one which takes the piss of arabs, but its too long\n",
      "\n",
      "Sentiment: [negative]\n",
      "\n",
      "\n",
      "Tweet Content: On train with at least two gaggles of teenagers sitting & the commuters squished standing in the back...at least the teenagers let me sit\n",
      "\n",
      "Sentiment: [neutral]\n",
      "\n",
      "\n",
      "Now, you are going to be given information for a single tweet.\n",
      "Tweet Content: had an answered prayer which caught me by surprise. LORD, you are amazing!\n",
      "Predict the sentiment class from positive/negative/neutral.\n",
      "Do not continue after making the prediction for this single tweet.\n",
      "\n",
      "Sentiment for 1 tweet: [positive]\n"
     ]
    }
   ],
   "source": [
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted = positive\n",
      "True Sentiment: [positive]\n"
     ]
    }
   ],
   "source": [
    "predicted_class = get_prediction(output['choices'][0]['text'])\n",
    "print(f\"Predicted = {predicted_class}\\nTrue {test_tweet_sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_gguf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
