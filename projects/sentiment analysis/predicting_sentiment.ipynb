{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27481, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = pd.read_csv('data/train.csv', encoding='unicode_escape')\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_for_llm(tweet_arr):\n",
    "    tweet_txt = tweet_arr[0][1]\n",
    "\n",
    "    tweet_info = f'''The tweet has the following information:\\n\n",
    "Tweet Content:\\n{tweet_txt}'''\n",
    "\n",
    "    return tweet_info\n",
    "\n",
    "# for providing N-shot examples for the LLM in the prompt\n",
    "def extract_sentiment_for_llm(tweet_arr):\n",
    "    relevant_tokens = tweet_arr[0][2]\n",
    "    sentiment = tweet_arr[0][3]\n",
    "\n",
    "    tweet_sentiment = f'''Sentiment for the above tweet is:\\n[{sentiment}]\\n\n",
    "Relevant text from the original tweet that leads to the conclusion that the sentiment in the tweet is [{sentiment}] are:\\n[{relevant_tokens}]\\n\\n\n",
    "Final answer: {sentiment}'''\n",
    "\n",
    "    return tweet_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['17e8450ef0', 'Tomorrow is house shopping...',\n",
       "        'Tomorrow is house shopping...', 'neutral', 'noon', '60-70',\n",
       "        \"CÃ´te d'Ivoire\", 26378274, 318000.0, 83]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tweets.sample(1).values\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet has the following information:\n",
      "\n",
      "Tweet Content:\n",
      "Tomorrow is house shopping...\n",
      "\n",
      "Sentiment for the above tweet is:\n",
      "[neutral]\n",
      "\n",
      "Relevant text from the original tweet that leads to the conclusion that the sentiment in the tweet is [neutral] are:\n",
      "[Tomorrow is house shopping...]\n",
      "\n",
      "\n",
      "Final answer: neutral\n"
     ]
    }
   ],
   "source": [
    "sample_tweet_content = extract_text_for_llm(sample)\n",
    "print(sample_tweet_content, end='\\n\\n')\n",
    "\n",
    "sample_tweet_sentiment = extract_sentiment_for_llm(sample)\n",
    "print(sample_tweet_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f'''You are an expert at linguistic analysis \\\n",
    "and your area of expertise is interpreting the sentiment of \\\n",
    "texts from tweets published online. You are very intelligent and \\\n",
    "are extremely good at your job. \n",
    "\n",
    "You task will go as follows: you will be provided \\\n",
    "with some information containing the text from a tweet.\\\n",
    "You will analyze it expertly and respond with 2 pieces of information.\n",
    "\n",
    "First, the sentiment of the tweet. It could be \\\n",
    "one of three options: [positive, negative, neutral].\\\n",
    "\n",
    "Secondly, you will point out the words \\\n",
    "from the original tweet that supports that sentiment inference \\\n",
    "in your expert and accurate analysis.'''\n",
    "\n",
    "one_shot_example = f'''\\n\\nFor example:-\\n\n",
    "{sample_tweet_content}\\n\\n\\\n",
    "{sample_tweet_sentiment}'''\n",
    "\n",
    "one_shot = True \n",
    "\n",
    "if one_shot:\n",
    "    system_prompt = system_prompt + one_shot_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at linguistic analysis and your area of expertise is interpreting the sentiment of texts from tweets published online. You are very intelligent and are extremely good at your job. \n",
      "\n",
      "You task will go as follows: you will be provided with some information containing the text from a tweet.You will analyze it expertly and respond with 2 pieces of information.\n",
      "\n",
      "First, the sentiment of the tweet. It could be one of three options: [positive, negative, neutral].\n",
      "Secondly, you will point out the words from the original tweet that supports that sentiment inference in your expert and accurate analysis.\n",
      "\n",
      "For example:-\n",
      "\n",
      "The tweet has the following information:\n",
      "\n",
      "Tweet Content:\n",
      "Tomorrow is house shopping...\n",
      "\n",
      "Sentiment for the above tweet is:\n",
      "[neutral]\n",
      "\n",
      "Relevant text from the original tweet that leads to the conclusion that the sentiment in the tweet is [neutral] are:\n",
      "[Tomorrow is house shopping...]\n",
      "\n",
      "\n",
      "Final answer: neutral\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from C:/Users/chinm/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "mpath = r'C:/Users/chinm/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf'\n",
    "\n",
    "llm = Llama(model_path=mpath, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['b4024682ca',\n",
       "        '  hope you guys are having fun!  Can`t wait for ya`ll to be back, Wilmy isn`t the same without you',\n",
       "        'g fun', 'positive', 'noon', '60-70', 'Chad', 16425864,\n",
       "        1259200.0, 13]], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = tweets.sample(1).values\n",
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet has the following information:\n",
      "\n",
      "Tweet Content:\n",
      "  hope you guys are having fun!  Can`t wait for ya`ll to be back, Wilmy isn`t the same without you\n",
      "\n",
      "Sentiment for the above tweet is:\n",
      "[positive]\n",
      "\n",
      "Relevant text from the original tweet that leads to the conclusion that the sentiment in the tweet is [positive] are:\n",
      "[g fun]\n",
      "\n",
      "\n",
      "Final answer: positive\n"
     ]
    }
   ],
   "source": [
    "test_tweet_content = extract_text_for_llm(test_sample)\n",
    "print(test_tweet_content, end='\\n\\n')\n",
    "\n",
    "test_tweet_sentiment = extract_sentiment_for_llm(test_sample)\n",
    "print(test_tweet_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9374.52 ms\n",
      "llama_print_timings:      sample time =      16.43 ms /    84 runs   (    0.20 ms per token,  5112.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21306.93 ms /   289 tokens (   73.73 ms per token,    13.56 tokens per second)\n",
      "llama_print_timings:        eval time =   18553.91 ms /    83 runs   (  223.54 ms per token,     4.47 tokens per second)\n",
      "llama_print_timings:       total time =   40115.72 ms /   372 tokens\n"
     ]
    }
   ],
   "source": [
    "question_prompt = system_prompt + \\\n",
    "    '\\n---------------------\\n' +\\\n",
    "    'Now, take a deep breath and answer this for the following tweet.\\n' +\\\n",
    "        test_tweet_content\n",
    "\n",
    "output = llm(\n",
    "    question_prompt, # Prompt\n",
    "    max_tokens= None, # Generate up to [max_tokens] tokens, set to None to generate up to the end of the context window\n",
    "    echo=True, # Echo the prompt back in the output\n",
    "    temperature=0 # The temperature parameter (higher for more \"creative\" or \"out of distribution\" output)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-07ad5dd6-4d69-4751-b00b-040d0a72f6e9',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1714526952,\n",
       " 'model': 'C:/Users/chinm/.cache/lm-studio/models/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_K_M.gguf',\n",
       " 'choices': [{'text': 'You are an expert at linguistic analysis and your area of expertise is interpreting the sentiment of texts from tweets published online. You are very intelligent and are extremely good at your job. \\n\\nYou task will go as follows: you will be provided with some information containing the text from a tweet.You will analyze it expertly and respond with 2 pieces of information.\\n\\nFirst, the sentiment of the tweet. It could be one of three options: [positive, negative, neutral].\\nSecondly, you will point out the words from the original tweet that supports that sentiment inference in your expert and accurate analysis.\\n\\nFor example:-\\n\\nThe tweet has the following information:\\n\\nTweet Content:\\nTomorrow is house shopping...\\n\\nSentiment for the above tweet is:\\n[neutral]\\n\\nRelevant text from the original tweet that leads to the conclusion that the sentiment in the tweet is [neutral] are:\\n[Tomorrow is house shopping...]\\n\\n\\nFinal answer: neutral\\n---------------------\\nNow, take a deep breath and answer this for the following tweet.\\nThe tweet has the following information:\\n\\nTweet Content:\\n  hope you guys are having fun!  Can`t wait for ya`ll to be back, Wilmy isn`t the same without you guys!\\n\\nSentiment for the above tweet is:\\n[positive]\\n\\nRelevant text from the original tweet that leads to the conclusion that the sentiment in the tweet is [positive] are:\\n[hope you guys are having fun!  Can`t wait for ya`ll to be back, Wilmy isn`t the same without you guys!]',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 290, 'completion_tokens': 83, 'total_tokens': 373}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert at linguistic analysis and your area of expertise is interpreting the sentiment of texts from tweets published online. You are very intelligent and are extremely good at your job. \n",
      "\n",
      "You task will go as follows: you will be provided with some information containing the text from a tweet.You will analyze it expertly and respond with 2 pieces of information.\n",
      "\n",
      "First, the sentiment of the tweet. It could be one of three options: [positive, negative, neutral].\n",
      "Secondly, you will point out the words from the original tweet that supports that sentiment inference in your expert and accurate analysis.\n",
      "\n",
      "For example:-\n",
      "\n",
      "The tweet has the following information:\n",
      "\n",
      "Tweet Content:\n",
      "Tomorrow is house shopping...\n",
      "\n",
      "Sentiment for the above tweet is:\n",
      "[neutral]\n",
      "\n",
      "Relevant text from the original tweet that leads to the conclusion that the sentiment in the tweet is [neutral] are:\n",
      "[Tomorrow is house shopping...]\n",
      "\n",
      "\n",
      "Final answer: neutral\n",
      "---------------------\n",
      "Now, take a deep breath and answer this for the following tweet.\n",
      "The tweet has the following information:\n",
      "\n",
      "Tweet Content:\n",
      "  hope you guys are having fun!  Can`t wait for ya`ll to be back, Wilmy isn`t the same without you guys!\n",
      "\n",
      "Sentiment for the above tweet is:\n",
      "[positive]\n",
      "\n",
      "Relevant text from the original tweet that leads to the conclusion that the sentiment in the tweet is [positive] are:\n",
      "[hope you guys are having fun!  Can`t wait for ya`ll to be back, Wilmy isn`t the same without you guys!]\n"
     ]
    }
   ],
   "source": [
    "print(output['choices'][0]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_gguf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
